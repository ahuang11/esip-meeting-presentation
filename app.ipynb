{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta code to wrap into slides\n",
    "# `panel serve app.ipynb --autoreload`\n",
    "\n",
    "import panel as pn\n",
    "\n",
    "pn.extension(\n",
    "    template='slides', css_files=[\n",
    "        'https://fonts.googleapis.com/css?family=Inter'\n",
    "    ], raw_css=[\n",
    "        'html, body { font-family: \"Inter\",sans-serif; }',\n",
    "        ':root {--r-background-color: rgb(16, 39, 47) }',\n",
    "        \"h1 { text-align: left; font-size: 3em; }\",\n",
    "        \"h2 { text-align: left; font-size: 3em; }\",\n",
    "        \"li { text-align: left; font-size: 2em; }\",\n",
    "        \"p { text-align: left; font-size: 2em; }\",\n",
    "        \"pre { text-align: left; }\",\n",
    "    ],\n",
    "    theme='dark'\n",
    ")\n",
    "\n",
    "pn.state.template.param.update(\n",
    "    design=pn.theme.Material,\n",
    "    header_background='white',\n",
    "    logo='anaconda.png',\n",
    "    title='Improve ML performance with hvplot',\n",
    ")\n",
    "\n",
    "pn.state.template.config.param.update(\n",
    "    raw_css=[\n",
    "        \"#header { height: 0; padding: 20px; }\",\n",
    "        \"li { text-align: left; }\",\n",
    "        \"p { margin-block-start: 0.5em; margin-block-end: 0.2em}\",\n",
    "    ],\n",
    "    css_files=[pn.io.resources.CSS_URLS['font-awesome']]\n",
    ")\n",
    "\n",
    "slide = lambda *objs: pn.Column(\n",
    "    pn.Column(*objs), sizing_mode='stretch_both', min_height=600, styles={\n",
    "        'display': 'flex', 'height': '100%', 'align-items': 'center'\n",
    "    }\n",
    ")\n",
    "\n",
    "def header(text, size='4em', **kwargs):\n",
    "    return HTML(f'<span>{text}</span>', styles={'font-size': size, 'font-weight': 'bold'}, **kwargs)\n",
    "\n",
    "def text_fragment(text, size='0.5em', **kwargs):\n",
    "    return Markdown(\n",
    "        text,\n",
    "        styles={'font-size': size, 'font-weight': 'bold'},\n",
    "        tags=['fragment'], **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def ends(df):\n",
    "    try:\n",
    "        return pn.Column(df.iloc[:5, :5], \"...\", df.iloc[-5:, -5:])\n",
    "    except:\n",
    "        return pn.Column(df.iloc[:5], \"...\", df.iloc[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide(\n",
    "    pn.Column(\n",
    "        \"\"\"\n",
    "        ## Improving ML performance with hvplot\n",
    "\n",
    "        - hvplot makes visualizing your data easy; simply set kwargs\n",
    "        \"\"\",\n",
    "        pn.Row(\n",
    "            pn.panel(\"katrina.png\", width=300),\n",
    "            pn.panel(\"tracks_climatology.png\", width=300)\n",
    "        ),\n",
    "    ).servable()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide(\n",
    "    pn.Column(\n",
    "        \"\"\"\n",
    "        ## Improving ML performance with hvplot\n",
    "\n",
    "        - find the most influential variables for feature selection\n",
    "        - help improve model performance\n",
    "        \"\"\",\n",
    "        pn.Row(\n",
    "            pn.panel(\"heatmap.png\", width=300),\n",
    "            pn.panel(\"scores.png\", width=300)\n",
    "        ),\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"wget -nc https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/csv/ibtracs.since1980.list.v04r00.csv\")\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        \"\"\"\n",
    "        ## Download the data\n",
    "\n",
    "        - Archive of tropical cyclone records (TC) since 1980\n",
    "        - Tracks, intensity, location, wind speed, central pressure, etc\n",
    "        ```python\n",
    "        wget -nc https://www.ncei.noaa.gov/data/international-best-track-archive-for-\n",
    "        climate-stewardship-ibtracs/v04r00/access/csv/ibtracs.since1980.list.v04r00.csv\n",
    "        ```\n",
    "        \"\"\"\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ibtracs.since1980.list.v04r00.csv\")\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        pn.Row(\n",
    "            \"\"\"\n",
    "            ## Read / show the data\n",
    "\n",
    "            - Ensure data as expected\n",
    "            - Use display for prettier tables\n",
    "            - All are `objects` (strings)\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            ```python\n",
    "            import pandas as pd\n",
    "\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.max_rows', None)\n",
    "\n",
    "            df = pd.read_csv(\"ibtracs.since1980.list.v04r00.csv\")\n",
    "            display(df.dtypes[:8], df.head())\n",
    "            ```\n",
    "            \"\"\",\n",
    "        ),\n",
    "        pn.Row(ends(df.dtypes), ends(df)),\n",
    "    ).servable()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ibtracs.since1980.list.v04r00.csv\", parse_dates=[\"ISO_TIME\"], skiprows=[1])\n",
    "\n",
    "icols = [19, 20, 161, 162]\n",
    "for icol in icols:\n",
    "    col = df.iloc[:, icol].name\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        \"\"\"\n",
    "        ## Clean the data\n",
    "\n",
    "        - Remove second header row\n",
    "        - Fix data types (object to numeric)\n",
    "\n",
    "        ```python\n",
    "        df = pd.read_csv(\"ibtracs.since1980.list.v04r00.csv\", parse_dates=[\"ISO_TIME\"], skiprows=[1])\n",
    "    \n",
    "        icols = [19, 20, 161, 162]\n",
    "        for icol in icols:\n",
    "            col = df.iloc[:, icol].name\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    \n",
    "        display(df.dtypes[:8], df.head())\n",
    "        ```\n",
    "        \"\"\",\n",
    "        pn.Row(ends(df.dtypes), ends(df)),\n",
    "    ).servable()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"ibtracs.parquet\")\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        \"\"\"\n",
    "        ## Convert the format\n",
    "\n",
    "        - Minimal effort for improved efficiency\n",
    "\n",
    "        ```python\n",
    "        df.to_parquet(\"ibtracs.parquet\")\n",
    "        ```\n",
    "        \"\"\",\n",
    "    ).servable()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import hvplot.pandas\n",
    "\n",
    "katrina_df = duckdb.execute(\n",
    "    \"\"\"\n",
    "    SELECT LON, LAT, USA_SSHS, USA_WIND, USA_PRES, ISO_TIME\n",
    "    FROM 'ibtracs.parquet'\n",
    "    WHERE NAME == 'KATRINA' AND SEASON == 2005\n",
    "    \"\"\"\n",
    ").fetchdf()\n",
    "\n",
    "katrina_points = katrina_df.hvplot.points(\n",
    "    x=\"LON\",\n",
    "    y=\"LAT\",\n",
    "    color=\"USA_SSHS\",\n",
    "    hover_cols=[\"USA_WIND\", \"USA_PRES\", \"ISO_TIME\"],\n",
    "    tiles=True,\n",
    "    aspect=1\n",
    ").opts(responsive=True)\n",
    "\n",
    "slide(\n",
    "    pn.Row(\n",
    "        '''\n",
    "        ## Plot Katrina track\n",
    "\n",
    "        - Using duckdb + SQL to read parquet\n",
    "        - Easier to type and read and more performant\n",
    "        - `fetchdf()` serializes into `pd.DataFrame`\n",
    "        - Effortlessly plot by setting keywords\n",
    "\n",
    "        ```python\n",
    "        import duckdb\n",
    "        import hvplot.pandas\n",
    "\n",
    "        katrina_df = duckdb.execute(\n",
    "            \"\"\"\n",
    "            SELECT LON, LAT, USA_SSHS, USA_WIND, USA_PRES, ISO_TIME\n",
    "            FROM 'ibtracs.parquet'\n",
    "            WHERE NAME == 'KATRINA' AND SEASON == 2005\n",
    "            \"\"\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        # equivalent to the above\n",
    "        # df = pd.read_parquet(\"ibtracs.parquet\")\n",
    "        # katrina_df = df.loc[\n",
    "        #   (df[\"NAME\"] == \"KATRINA\") &\n",
    "        #   (df[\"SEASON\"] == 2005),\n",
    "        #   [\"LON\", \"LAT\", \"USA_SSHS\", \"USA_WIND\", \"USA_PRES\", \"ISO_TIME\"]\n",
    "        # ]\n",
    "\n",
    "        katrina_df.hvplot.points(\n",
    "            x=\"LON\",\n",
    "            y=\"LAT\",\n",
    "            color=\"USA_SSHS\",\n",
    "            hover_cols=[\"USA_WIND\", \"USA_PRES\", \"ISO_TIME\"],\n",
    "            tiles=True,\n",
    "        )\n",
    "        ```\n",
    "        ''',\n",
    "    ).servable()\n",
    ")\n",
    "\n",
    "\n",
    "slide(\n",
    "    pn.pane.HoloViews(katrina_points).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "\n",
    "df = duckdb.query(\n",
    "    \"\"\"\n",
    "    SELECT LON, LAT FROM 'ibtracs.parquet'\n",
    "    \"\"\"\n",
    ").fetchdf()\n",
    "\n",
    "climatology_points = df.hvplot.points(\n",
    "    x=\"LON\",\n",
    "    y=\"LAT\",\n",
    "    cmap=\"viridis\",\n",
    "    rasterize=True,\n",
    "    tiles=True,\n",
    "    x_sampling=1,\n",
    "    y_sampling=1,\n",
    "    aspect=1,\n",
    ").opts(xlim=(-179, 179), responsive=True)\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        '''\n",
    "        ## Plot tracks climatology\n",
    "\n",
    "        - `rasterize` can visualize billions of points\n",
    "        - Prevents overplotting--converts into 2D histogram\n",
    "\n",
    "        ```python\n",
    "        import duckdb\n",
    "        import hvplot.pandas\n",
    "\n",
    "        df = duckdb.query(\n",
    "            \"\"\"\n",
    "            SELECT LON, LAT FROM 'ibtracs.parquet'\n",
    "            \"\"\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        df.hvplot.points(\n",
    "            x=\"LON\",\n",
    "            y=\"LAT\",\n",
    "            cmap=\"viridis\",\n",
    "            rasterize=True,\n",
    "            coastline=True,\n",
    "            global_extent=True,\n",
    "        )\n",
    "        ```\n",
    "        ''',\n",
    "    ).servable()\n",
    ")\n",
    "\n",
    "slide(\n",
    "    pn.pane.HoloViews(climatology_points).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide(\n",
    "    pn.Row(\n",
    "        '''\n",
    "        ## Initial motivation\n",
    "\n",
    "        \"El Niño events generally suppress Atlantic hurricane<br>activity\n",
    "        with fewer hurricanes than normal in the Atlantic basin<br>\n",
    "        during the peak of Atlantic hurricane season\"\n",
    "        [[Source](https://www.weather.gov/jan/el_nino_and_la_nina)].\n",
    "        ''',\n",
    "        pn.panel(\"https://www.weather.gov/images/jan/ElNino_LaNina/elninowxpatterns.gif\"),\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_df = duckdb.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM ibtracs.parquet\n",
    "    WHERE\n",
    "        NAME != 'NOT_NAMED' AND\n",
    "        USA_ATCF_ID LIKE 'AL%' AND\n",
    "        SEASON < 2023 AND\n",
    "        DATE_PART('month', ISO_TIME) >= 6 AND\n",
    "        DATE_PART('month', ISO_TIME) <= 11\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "atlantic_points = atlantic_df.hvplot.points(\n",
    "    x=\"LON\",\n",
    "    y=\"LAT\",\n",
    "    cmap=\"viridis\",\n",
    "    rasterize=True,\n",
    "    tiles=True,\n",
    "    x_sampling=1,\n",
    "    y_sampling=1,\n",
    "    aspect=1,\n",
    ").opts(xlim=(-179, 179), responsive=True)\n",
    "\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        '''\n",
    "        ## Select Atlantic hurricane season\n",
    "\n",
    "        - The six-month season runs from June 1 to November 30 [[Source](https://www.noaa.gov/news-release/2023-atlantic-hurricane-season-outlook)].\n",
    "        - Subset named hurricanes in the Atlantic basin during season only\n",
    "        - Verify queried as expected by visualizing\n",
    "\n",
    "        ```python\n",
    "        atlantic_df = duckdb.execute(\"\"\"\n",
    "            SELECT *\n",
    "            FROM ibtracs.parquet\n",
    "            WHERE\n",
    "                NAME != 'NOT_NAMED' AND\n",
    "                USA_ATCF_ID LIKE 'AL%' AND\n",
    "                SEASON < 2023 AND\n",
    "                DATE_PART('month', ISO_TIME) >= 6 AND\n",
    "                DATE_PART('month', ISO_TIME) <= 11\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        # equivalent to the above\n",
    "        # df = pd.read_parquet(\"ibtracs.parquet\")\n",
    "        # atlantic_df = df.loc[\n",
    "        #   (df[\"USA_SSHS\"] > 0) &\n",
    "        #   (df[\"USA_ATCF_ID\"].str.startswith(\"AL\"))\n",
    "        # ]\n",
    "\n",
    "        atlantic_df.hvplot.points(\n",
    "            x=\"LON\",\n",
    "            y=\"LAT\",\n",
    "            cmap=\"viridis\",\n",
    "            rasterize=True,\n",
    "            coastline=True,\n",
    "            x_sampling=1,\n",
    "            y_sampling=1,\n",
    "        )\n",
    "        ```\n",
    "        ''',\n",
    "    ).servable()\n",
    ")\n",
    "\n",
    "slide(\n",
    "    pn.pane.HoloViews(atlantic_points).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_count_df = duckdb.execute(\"\"\"\n",
    "    SELECT SEASON, COUNT(DISTINCT NAME) AS unique_names\n",
    "    FROM ibtracs.parquet\n",
    "    WHERE\n",
    "        NAME != 'NOT_NAMED' AND\n",
    "        USA_ATCF_ID LIKE 'AL%' AND\n",
    "        SEASON < 2023 AND\n",
    "        DATE_PART('month', ISO_TIME) >= 6 AND\n",
    "        DATE_PART('month', ISO_TIME) <= 11\n",
    "    GROUP BY SEASON;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "count_plot = atlantic_count_df.hvplot(\"SEASON\", \"unique_names\")\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        '''\n",
    "        ## Calculate Atlantic hurricane season count\n",
    "\n",
    "        - Names are unique per season (increments by alphabet)\n",
    "\n",
    "        ```python\n",
    "        atlantic_count_df = duckdb.execute(\"\"\"\n",
    "            SELECT SEASON, COUNT(DISTINCT NAME) AS unique_names\n",
    "            FROM ibtracs.parquet\n",
    "            WHERE\n",
    "                NAME != 'NOT_NAMED' AND\n",
    "                USA_ATCF_ID LIKE 'AL%' AND\n",
    "                SEASON < 2023 AND\n",
    "                DATE_PART('month', ISO_TIME) >= 6 AND\n",
    "                DATE_PART('month', ISO_TIME) <= 11\n",
    "            GROUP BY SEASON;\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        atlantic_count_df.hvplot(\"SEASON\", \"unique_names\")\n",
    "        ''',\n",
    "        pn.pane.HoloViews(count_plot)\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nino_df = pd.read_csv('https://raw.githubusercontent.com/ahuang11/oni/master/nino_ml.csv', index_col=0, parse_dates=True)\n",
    "nino_df = nino_df.rename_axis(\"date\").dropna()\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        '''\n",
    "        ## Get data relevant to Nino\n",
    "\n",
    "        - t300 - depth averaged temps up from 0 to 300m\n",
    "        - wwv - warm water volume\n",
    "        - u850 - 850 mb trade wind index\n",
    "        - *_e - east\n",
    "        - *_w - west\n",
    "        - *_c - central\n",
    "        - *_anom - anomaly\n",
    "        - *_norm - standardized\n",
    "\n",
    "        ```python\n",
    "        nino_df = pd.read_csv('https://raw.githubusercontent.com/ahuang11/oni/master/nino_ml.csv', index_col=0, parse_dates=True)\n",
    "        nino_df = nino_df.rename_axis(\"date\").dropna()\n",
    "\n",
    "        display(nino_df.head())\n",
    "        ''',\n",
    "        ends(nino_df),\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nino_df.to_parquet(\"nino.parquet\")\n",
    "\n",
    "nino_spring_df = duckdb.execute(\n",
    "    \"\"\"\n",
    "    SELECT *, EXTRACT(MONTH FROM date) AS month, EXTRACT(YEAR FROM date) AS year\n",
    "    FROM nino.parquet\n",
    "    WHERE EXTRACT(MONTH FROM date) BETWEEN 1 AND 5;\n",
    "    \"\"\"\n",
    ").fetchdf()\n",
    "\n",
    "slide(\n",
    "    pn.Row(\n",
    "        '''\n",
    "        ## Subset predictors\n",
    "        \n",
    "        - Select January to May (prior to peak season)\n",
    "\n",
    "        ```python\n",
    "        nino_df.to_parquet(\"nino.parquet\")\n",
    "\n",
    "        nino_spring_df = duckdb.execute(\n",
    "            \"\"\"\n",
    "            SELECT *, EXTRACT(MONTH FROM date) AS month, EXTRACT(YEAR FROM date) AS year\n",
    "            FROM nino.parquet\n",
    "            WHERE EXTRACT(MONTH FROM date) BETWEEN 1 AND 5;\n",
    "            \"\"\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        display(nino_spring_df.head())\n",
    "        ''',\n",
    "        pn.Column(\n",
    "            pn.panel(\"diagram.png\", width=500),\n",
    "            ends(nino_spring_df),\n",
    "        )\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = (\n",
    "    atlantic_count_df.set_index(\"SEASON\").join(nino_spring_df.set_index(\"year\"))\n",
    ").reset_index(names=[\"year\"])\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        '''\n",
    "        ## Combine the two dataframes\n",
    "\n",
    "        - Use year as the primary key\n",
    "\n",
    "        ```python\n",
    "        ml_df = (\n",
    "            atlantic_count_df.set_index(\"SEASON\").join(nino_spring_df.set_index(\"year\"))\n",
    "        ).reset_index(names=[\"year\"])\n",
    "        \n",
    "        display(ml_df.head())\n",
    "        ```\n",
    "        ''',\n",
    "        ends(ml_df),\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = (\n",
    "    ml_df.groupby(\"month\")\n",
    "    .corr(numeric_only=True)[\"unique_names\"]\n",
    "    .sort_values()\n",
    "    .rename_axis([\"month\", \"parameter\"])\n",
    ")\n",
    "\n",
    "corr_heatmap = corr_df.hvplot.heatmap(\n",
    "    \"month\",\n",
    "    \"parameter\",\n",
    "    \"unique_names\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    colorbar=True,\n",
    "    symmetric=True,\n",
    "    height=800,\n",
    ").opts(color_levels=12)\n",
    "\n",
    "slide(\n",
    "    pn.Row(\n",
    "        '''\n",
    "        # Explore features\n",
    "\n",
    "        - Feature selection helps performance\n",
    "        - Select the ones with highest correlation\n",
    "        - Nino indices are not extremely correlated\n",
    "        - Zonal winds at 850mb are quite correlated\n",
    "\n",
    "        ```python\n",
    "        corr_df = (\n",
    "            ml_df.groupby(\"month\")\n",
    "            .corr(numeric_only=True)[\"unique_names\"]\n",
    "            .sort_values()\n",
    "            .rename_axis([\"month\", \"parameter\"])\n",
    "        )\n",
    "        corr_df.hvplot.heatmap(\n",
    "            \"month\",\n",
    "            \"parameter\",\n",
    "            \"unique_names\",\n",
    "            cmap=\"RdBu_r\",\n",
    "            colorbar=True,\n",
    "            symmetric=True,\n",
    "            height=800,\n",
    "        ).opts(color_levels=12)\n",
    "        ```\n",
    "        ''',\n",
    "        pn.panel(corr_heatmap)\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_month_dfs = []\n",
    "\n",
    "for month in range(4, 6):\n",
    "    ml_month_df = ml_df.loc[ml_df[\"month\"] == month].drop(columns=[\"month\"])\n",
    "    ml_month_df = ml_month_df.set_index([\"year\", \"unique_names\"])\n",
    "    ml_month_df.columns = ml_month_df.columns + f\"_m{month}\"\n",
    "    ml_month_dfs.append(ml_month_df)\n",
    "\n",
    "ml_month_df = pd.concat(ml_month_dfs, axis=1).reset_index()\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        '''\n",
    "        ## Re-orient the data\n",
    "\n",
    "        - Convert months into separate predictor columns\n",
    "\n",
    "        ```python\n",
    "        ml_month_dfs = []\n",
    "        for month in range(4, 6):\n",
    "            ml_month_df = ml_df.loc[ml_df[\"month\"] == month].drop(columns=[\"month\"])\n",
    "            ml_month_df = ml_month_df.set_index([\"year\", \"unique_names\"])\n",
    "            ml_month_df.columns = ml_month_df.columns + f\"_m{month}\"\n",
    "            ml_month_dfs.append(ml_month_df)\n",
    "\n",
    "        ml_month_df = pd.concat(ml_month_dfs, axis=1).reset_index()\n",
    "        ```\n",
    "        ''',\n",
    "        ends(ml_month_df),\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_param_df = ml_month_df.filter(\n",
    "    regex=\"t300_w|u850_c|olr|t300_e|wwv_e|year|month|unique_names\"\n",
    ")\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        '''\n",
    "        ## Subset the best features\n",
    "\n",
    "        - Rather than manually typing each specific column name, filter by regex\n",
    "        ```python\n",
    "        ml_param_df = ml_month_df.filter(\n",
    "            regex=\"t300_w|u850_c|olr|t300_e|wwv_e|year|month|unique_names\"\n",
    "        )\n",
    "\n",
    "        display(ml_param_df.head())\n",
    "        ```\n",
    "        ''',\n",
    "        ends(ml_param_df),\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def run_model(model, X, y, train_index, val_index, **model_kwargs):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    model = model(y_train, X_train, **model_kwargs)\n",
    "    fitted_model = model.fit()\n",
    "\n",
    "    X_val = sm.add_constant(X_val)\n",
    "    y_pred_val = fitted_model.predict(X_val)\n",
    "\n",
    "    val_df = pd.DataFrame(\n",
    "        {\"year\": X_val[\"year\"], \"actual\": y_val, \"prediction\": y_pred_val}\n",
    "    ).sort_values(\"year\")\n",
    "    return val_df\n",
    "\n",
    "def score_output(val_df):\n",
    "    corr = val_df[\"actual\"].corr(val_df[\"prediction\"])\n",
    "    rmse = ((val_df[\"actual\"] - val_df[\"prediction\"]) ** 2).mean() ** 0.5\n",
    "    return pd.Series([corr, rmse], index=[\"corr\", \"rmse\"])\n",
    "\n",
    "slide(\n",
    "    pn.Row(\n",
    "        '''\n",
    "        ## Create reusable ML functions\n",
    "\n",
    "        - Splits dataset into training and validation\n",
    "        - Fits the model\n",
    "        - Makes prediction\n",
    "        - Stores output for scoring\n",
    "        ''',\n",
    "        '''\n",
    "        ```python\n",
    "        import statsmodels.api as sm\n",
    "\n",
    "        def run_model(model, X, y, train_index, val_index, **model_kwargs):\n",
    "            X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "            X_train = sm.add_constant(X_train)\n",
    "            model = model(y_train, X_train, **model_kwargs)\n",
    "            fitted_model = model.fit()\n",
    "\n",
    "            X_val = sm.add_constant(X_val)\n",
    "            y_pred_val = fitted_model.predict(X_val)\n",
    "\n",
    "            val_df = pd.DataFrame(\n",
    "                {\"year\": X_val[\"year\"], \"actual\": y_val, \"prediction\": y_pred_val}\n",
    "            ).sort_values(\"year\")\n",
    "            return val_df\n",
    "\n",
    "\n",
    "        def score_output(val_df):\n",
    "            corr = val_df[\"actual\"].corr(val_df[\"prediction\"])\n",
    "            rmse = ((val_df[\"actual\"] - val_df[\"prediction\"]) ** 2).mean() ** 0.5\n",
    "            return pd.Series([corr, rmse], index=[\"corr\", \"rmse\"])\n",
    "        ```\n",
    "        ''',\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = ml_param_df.drop(columns=[\"unique_names\"])\n",
    "y = ml_param_df[\"unique_names\"]\n",
    "\n",
    "num_folds = 4  # Adjust the number of folds as needed\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "val_dfs = []\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    val_df = run_model(sm.OLS, X, y, train_index, val_index)\n",
    "    val_df[\"fold\"] = i\n",
    "    val_dfs.append(val_df)\n",
    "\n",
    "val_df = pd.concat(val_dfs).sort_values(\"year\")\n",
    "\n",
    "score_df = (val_df.groupby(\"fold\").apply(score_output))\n",
    "val_plot = val_df.hvplot(\"year\", [\"actual\", \"prediction\"], title=\"With Feature Selection\")\n",
    "\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        '''\n",
    "        # Run linear regression and score\n",
    "\n",
    "        - Split dataset into X (predictors) and Y (goal)\n",
    "        - Run K-Fold for reliability and preventing overfitting\n",
    "\n",
    "        ```python\n",
    "        from sklearn.model_selection import KFold\n",
    "\n",
    "        X = ml_param_df.drop(columns=[\"unique_names\"])\n",
    "        y = ml_param_df[\"unique_names\"]\n",
    "\n",
    "        num_folds = 4  # Adjust the number of folds as needed\n",
    "        kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "        val_dfs = []\n",
    "        for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "            val_df = run_model(sm.OLS, X, y, train_index, val_index)\n",
    "            val_df[\"fold\"] = i\n",
    "            val_dfs.append(val_df)\n",
    "\n",
    "        val_df = pd.concat(val_dfs).sort_values(\"year\")\n",
    "\n",
    "        score_df = (val_df.groupby(\"fold\").apply(score_output))\n",
    "        val_plot = val_df.hvplot(\"year\", [\"actual\", \"prediction\"])\n",
    "        display(val_plot, score_df)\n",
    "        ```\n",
    "        ''',\n",
    "    ).servable()\n",
    ")\n",
    "\n",
    "slide(\n",
    "    pn.Row(\n",
    "        val_plot, score_df,\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ml_month_df.drop(columns=[\"unique_names\"]).select_dtypes(\"number\")\n",
    "y = ml_month_df[\"unique_names\"]\n",
    "\n",
    "num_folds = 4  # Adjust the number of folds as needed\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "val_dfs = []\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    val_df = run_model(sm.OLS, X, y, train_index, val_index)\n",
    "    val_df[\"fold\"] = i\n",
    "    val_dfs.append(val_df)\n",
    "\n",
    "val_df = pd.concat(val_dfs).sort_values(\"year\")\n",
    "\n",
    "new_score_df = (val_df.groupby(\"fold\").apply(score_output))\n",
    "new_val_plot = val_df.hvplot(\"year\", [\"actual\", \"prediction\"], title=\"No Feature Selection\")\n",
    "\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        '''\n",
    "        ## Re-run and score w/o feature selection\n",
    "\n",
    "        - Other features are noise\n",
    "        - This lowers the skill score\n",
    "\n",
    "        ```python\n",
    "        X = ml_month_df.drop(columns=[\"unique_names\"]).select_dtypes(\"number\")\n",
    "        y = ml_month_df[\"unique_names\"]\n",
    "\n",
    "        num_folds = 4  # Adjust the number of folds as needed\n",
    "        kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "        val_dfs = []\n",
    "        for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "            val_df = run_model(sm.OLS, X, y, train_index, val_index)\n",
    "            val_df[\"fold\"] = i\n",
    "            val_dfs.append(val_df)\n",
    "\n",
    "        val_df = pd.concat(val_dfs).sort_values(\"year\")\n",
    "\n",
    "        score_df = (val_df.groupby(\"fold\").apply(score_output))\n",
    "        val_plot = val_df.hvplot(\"year\", [\"actual\", \"prediction\"])\n",
    "        display(val_plot, score_df)\n",
    "        ```\n",
    "        ''',\n",
    "    ).servable()\n",
    ")\n",
    "\n",
    "slide(\n",
    "    pn.Column(\n",
    "        pn.Row(\n",
    "            val_plot, score_df,\n",
    "        ),\n",
    "        pn.Row(\n",
    "            new_val_plot, new_score_df,\n",
    "        )\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide(\n",
    "    pn.Row(\n",
    "        '''\n",
    "        ## Takeaways\n",
    "\n",
    "        - hvplot is super easy to use\n",
    "        - Feature selection can help with model performance\n",
    "        ''',\n",
    "    ).servable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide(\n",
    "    pn.Row(\n",
    "        '''\n",
    "        ## Ideas to try\n",
    "\n",
    "        - Different models (deep learning?)\n",
    "        - Using other predictors (other oscillations)\n",
    "        - Feature engineering (previous year's count)\n",
    "        ''',\n",
    "    ).servable()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
